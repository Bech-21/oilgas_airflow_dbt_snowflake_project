name: CD

on:
  push:
    branches: [ main ]

jobs:
  deploy:
    runs-on: ubuntu-latest

    steps:
      # ------------------------
      # Checkout repo
      # ------------------------
      - name: Checkout repository
        uses: actions/checkout@v4

      # ------------------------
      # Python setup
      # ------------------------
      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.10"

      # ------------------------
      # Install dependencies (Airflow + dbt)
      # ------------------------
      - name: Install Airflow and dbt
        run: |
          python -m pip install --upgrade pip
          pip install "apache-airflow==2.7.3" dbt-snowflake Flask-Session==0.5.0 SQLAlchemy==1.4.46

      # ------------------------
      # Setup dbt profile
      # ------------------------
      - name: Setup dbt profile
        run: |
          mkdir -p ~/.dbt
          cat > ~/.dbt/profiles.yml <<EOL
          oilandgas_dbt:
            target: prod
            outputs:
              prod:
                type: snowflake
                account: ${{ secrets.SNOWFLAKE_ACCOUNT }}
                user: ${{ secrets.SNOWFLAKE_USER }}
                password: ${{ secrets.SNOWFLAKE_PASSWORD }}
                role: ACCOUNTADMIN
                database: OILGAS_DB
                warehouse: ${{ secrets.SNOWFLAKE_WAREHOUSE }}
                schema: raw
          EOL

      # ------------------------
      # Initialize Airflow DB (with proper migration)
      # ------------------------
      - name: Initialize Airflow environment
        run: |
          export AIRFLOW_HOME=$(pwd)/airflow
          mkdir -p $AIRFLOW_HOME/dags
          cp -r dags/* $AIRFLOW_HOME/dags/
          
          # Use db migrate instead of db init
          airflow db migrate
          
          # Create admin user
          airflow users create \
            --username admin \
            --firstname Air \
            --lastname Flow \
            --role Admin \
            --email admin@example.com \
            --password admin

      # ------------------------
      # Validate DAGs (syntax check)
      # ------------------------
      - name: Validate DAG syntax
        run: |
          python -m py_compile dags/ELT_pipeline.py
          python -m py_compile dags/dbt_airflow_cmd.py
          echo "✓ DAG syntax validation successful"

      # ------------------------
      # Test DAG imports
      # ------------------------
      - name: Test DAG imports
        env:
          AIRFLOW__CORE__LOAD_EXAMPLES: "false"
          AIRFLOW__CORE__LOAD_DEFAULT_CONNECTIONS: "false"
        run: |
          export AIRFLOW_HOME=$(pwd)/airflow
          python -c "
          import os
          import sys
          sys.path.insert(0, 'dags')
          try:
              # Test importing DAG files
              from ELT_pipeline import dag as etl_dag
              from dbt_airflow_cmd import dag as dbt_dag
              print('✓ Successfully imported ETL_pipeline DAG')
              print('✓ Successfully imported dbt_airflow_cmd DAG')
              
              # Test pipeline module imports
              sys.path.insert(0, '.')
              from pipeline.bronze_layer import bronze_etl
              from pipeline.bronze_counties_layer import counties_bronze_etl
              from pipeline.bronze_earthquake_layer import earthquake_bronze_etl
              print('✓ Successfully imported all pipeline modules')
              
          except Exception as e:
              print(f'✗ Error: {e}')
              import traceback
              traceback.print_exc()
              sys.exit(1)
          "

      # ------------------------
      # List DAGs to verify they're loaded
      # ------------------------
      - name: List Airflow DAGs
        run: |
          export AIRFLOW_HOME=$(pwd)/airflow
          airflow dags list

      # ------------------------
      # Deploy dbt project
      # ------------------------
      - name: Deploy dbt project
        run: |
          cd oilandgas_dbt
          dbt deps
          dbt run
          dbt test

      # ------------------------
      # Test ETL DAG (dry run)
      # ------------------------
      - name: Test ETL DAG structure
        run: |
          export AIRFLOW_HOME=$(pwd)/airflow
          airflow dags list | grep -q "ETL_pipeline" && echo "✓ ETL_pipeline DAG found"
          airflow dags list | grep -q "running_oilandgas_dbt" && echo "✓ running_oilandgas_dbt DAG found"